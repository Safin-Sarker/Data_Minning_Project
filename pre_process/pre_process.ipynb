{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import os\n",
    "import csv\n",
    "\n",
    "class TweetDataLoader:\n",
    "    def __init__(self, data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        self.total_link_count = 0\n",
    "        self.total_word_with_num_count = 0\n",
    "        self.total_whitespace_removed_count = 0\n",
    "        self.total_punctuation_removed_count = 0\n",
    "        self.total_emoji_removed_count = 0\n",
    "\n",
    "    def load_data(self, file_name, columns):\n",
    "        file_path = os.path.join(self.data_folder, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\t', names=columns, skiprows=1)\n",
    "            return df\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error parsing file {file_path}: {e}\")\n",
    "            print(\"Attempting to skip problematic lines and continue loading...\")\n",
    "            df = self.skip_problematic_lines(file_path, columns)\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "\n",
    "    def skip_problematic_lines(self, file_path, columns):\n",
    "        lines = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            header = file.readline().strip()  # Read the first line (header)\n",
    "            for line in file:\n",
    "                if line.strip() == header:\n",
    "                    continue  # Skip the row if it matches the header\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) == len(columns):\n",
    "                    lines.append(fields)\n",
    "                else:\n",
    "                    print(f\"Skipping line with unexpected number of fields: {line.strip()}\")\n",
    "        df = pd.DataFrame(lines, columns=columns)\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df, text_column):\n",
    "        if df is None:\n",
    "            return None\n",
    "        df[text_column] = df[text_column].apply(self.preprocess_text)\n",
    "        if 'tweet_url' in df.columns:  # Check if 'tweet_url' column exists\n",
    "            df = df.drop(columns=['tweet_url'])  # Remove the 'tweet_url' column\n",
    "        return df, self.get_total_counts()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        capital_word_count = 0\n",
    "        link_count = 0\n",
    "        word_with_num_count = 0\n",
    "        whitespace_removed_count = 0\n",
    "        punctuation_removed_count = 0\n",
    "        emoji_removed_count = 0\n",
    "\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        text_lower = text.lower()\n",
    "        text, link_count = re.subn(r'https?:\\/\\/\\S+', '', text)\n",
    "        text, punctuation_removed_count = re.subn(r'[^\\w\\s]', '', text)\n",
    "        text, word_with_num_count = re.subn(r'\\w*\\d\\w*', '', text)\n",
    "        text, whitespace_removed_count = re.subn(r'\\s+', ' ', text.strip())\n",
    "\n",
    "        # Remove emojis\n",
    "        text, emoji_removed_count = self.remove_emojis(text)\n",
    "\n",
    "        self.total_link_count += link_count\n",
    "        self.total_word_with_num_count += word_with_num_count\n",
    "        self.total_whitespace_removed_count += whitespace_removed_count\n",
    "        self.total_punctuation_removed_count += punctuation_removed_count\n",
    "        self.total_emoji_removed_count += emoji_removed_count\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_emojis(self, text):\n",
    "        # Remove emojis using regular expressions\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\"\n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\"  \n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text), len(emoji_pattern.findall(text))\n",
    "\n",
    "    def get_total_counts(self):\n",
    "        counts_dict = {\n",
    "            \"Link count\": [self.total_link_count],\n",
    "            \"Word remove with number count\": [self.total_word_with_num_count],\n",
    "            \"Whitespace removed count\": [self.total_whitespace_removed_count],\n",
    "            \"Punctuation removed count\": [self.total_punctuation_removed_count],\n",
    "            \"Emoji removed count\": [self.total_emoji_removed_count]\n",
    "        }\n",
    "        return pd.DataFrame(counts_dict)\n",
    "\n",
    " \n",
    "    def preprocess_and_save(data_loader, file_name, columns, text_column):\n",
    "        df = data_loader.load_data(file_name, columns)\n",
    "        if df is not None:\n",
    "            df_preprocessed, total_counts_df = data_loader.preprocess_data(df, text_column)\n",
    "            if df_preprocessed is not None:\n",
    "                output_file_name = file_name.replace('.tsv', '_preprocessed.csv')\n",
    "                with open(output_file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(['tweet_id', 'tweet_text', 'class_label'])  # Write predefined column names\n",
    "                    for index, row in df_preprocessed.iterrows():\n",
    "                        writer.writerow(row)\n",
    "                print(f\"Preprocessed file saved: {output_file_name}\")\n",
    "\n",
    "              \n",
    "                print(\"Total Counts:\")\n",
    "                print(total_counts_df)\n",
    "                \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "data_folder = \"C:\\\\Users\\\\ASUS\\\\Downloads\\\\Data minning project\\\\Data_Minning_Project\\\\Preprocessing\"\n",
    "\n",
    "loader = TweetDataLoader(data_folder)\n",
    "\n",
    "# English data\n",
    "english_folder = os.path.join(data_folder, \"english\")\n",
    "english_columns = ['Sentence_id', 'Text', 'class_label']\n",
    "for file_name in os.listdir(english_folder):\n",
    "    if file_name.endswith(\".tsv\"):\n",
    "        TweetDataLoader.preprocess_and_save(loader, os.path.join(\"english\", file_name), english_columns, 'Text')\n",
    "\n",
    "# Spanish data\n",
    "spanish_folder = os.path.join(data_folder, \"spanish\")\n",
    "spanish_columns = ['tweet_id', 'tweet_url', 'tweet_text', 'class_label']\n",
    "for file_name in os.listdir(spanish_folder):\n",
    "    if file_name.endswith(\".tsv\"):\n",
    "        TweetDataLoader.preprocess_and_save(loader, os.path.join(\"spanish\", file_name), spanish_columns, 'tweet_text')\n",
    "\n",
    "# Dutch data\n",
    "dutch_folder = os.path.join(data_folder, \"dutch\")\n",
    "dutch_columns = ['tweet_id', 'tweet_url', 'tweet_text', 'class_label']\n",
    "for file_name in os.listdir(dutch_folder):\n",
    "    if file_name.endswith(\".tsv\"):\n",
    "        TweetDataLoader.preprocess_and_save(loader, os.path.join(\"dutch\", file_name), dutch_columns, 'tweet_text')\n",
    "\n",
    "# Arabic data\n",
    "arabic_folder = os.path.join(data_folder, \"arabic\")\n",
    "arabic_columns = ['tweet_id', 'tweet_url', 'tweet_text', 'class_label']\n",
    "for file_name in os.listdir(arabic_folder):\n",
    "    if file_name.endswith(\".tsv\"):\n",
    "        TweetDataLoader.preprocess_and_save(loader, os.path.join(\"arabic\", file_name), arabic_columns, 'tweet_text')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
