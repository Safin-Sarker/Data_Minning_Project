{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class TweetDataLoader:\n",
    "    def init(self, data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        self.total_link_count = 0\n",
    "        self.total_word_with_num_count = 0\n",
    "        self.total_whitespace_removed_count = 0\n",
    "        self.total_punctuation_removed_count = 0\n",
    "        self.total_non_english_alphabet_removed_count = 0\n",
    "    \n",
    "    def load_data(self, file_name):\n",
    "        file_path = f\"{self.data_folder}/{file_name}\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\\\t')\n",
    "            return df\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error parsing file {file_path}: {e}\")\n",
    "            print(\"Attempting to skip problematic lines and continue loading...\")\n",
    "            df = self.skip_problematic_lines(file_path)\n",
    "            return df\n",
    "\n",
    "def skip_problematic_lines(self, file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        first_line = file.readline().strip().split('\\\\t')\n",
    "        for line in file:\n",
    "            try:\n",
    "                fields = line.strip().split('\\\\t')\n",
    "                if len(fields) == len(first_line):  # Assuming all lines have the same number of fields as the first line\n",
    "                    lines.append(fields)\n",
    "                else:\n",
    "                    print(f\"Skipping line with unexpected number of fields: {line.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {line.strip()}. Skipping...\")\n",
    "                print(f\"Error details: {e}\")\n",
    "    df = pd.DataFrame(lines, columns=first_line)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(self, df):\n",
    "    # Preprocess the text in the 'Text' column\n",
    "    df['Text'] = df['Text'].apply(lambda text: self.preprocess_text(text))\n",
    "\n",
    "    # Calculate and print total counts\n",
    "    self.print_total_counts()\n",
    "\n",
    "    null_values_text = df['Text'].isnull().sum()\n",
    "    null_values_Sentence_id = df['Sentence_id'].isnull().sum()\n",
    "    null_values_cclass_label = df['class_label'].isnull().sum()\n",
    "\n",
    "# Print information about null values\n",
    "    print(f\"Null values in 'Text' column after preprocessing: {null_values_text}\")\n",
    "    print(f\"Null values in 'Sentence_id' column after preprocessing: {null_values_Sentence_id}\")\n",
    "    print(f\"Null values in 'class_label' column after preprocessing: {null_values_cclass_label}\")\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_text(self, text):\n",
    "# Initialize counts\n",
    "    capital_word_count = 0\n",
    "    link_count = 0\n",
    "    word_with_num_count = 0\n",
    "    whitespace_removed_count = 0\n",
    "    punctuation_removed_count = 0\n",
    "    non_english_alphabet_removed_count = 0\n",
    "\n",
    "    # Remove brackets and content within them\n",
    "    text = re.sub(r'\\\\([^)]*\\\\)', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text_lower = text.lower()\n",
    "    # Remove links and count them\n",
    "    text, link_count = re.subn(r'https?:\\\\/\\\\/\\\\S+', '', text)\n",
    "    # Remove punctuation and count them\n",
    "    text, punctuation_removed_count = re.subn(r'[^\\\\w\\\\s]', '', text)\n",
    "    # Remove words containing numbers and count them\n",
    "    text, word_with_num_count = re.subn(r'\\\\w*\\\\d\\\\w*', '', text)\n",
    "    # Remove extra whitespaces and count them\n",
    "    text, whitespace_removed_count = re.subn(r'\\\\s+', ' ', text.strip())\n",
    "     # Remove non-English alphabetic characters and count them\n",
    "    non_english_alphabet_removed_count = len(text) - len(re.sub(r'[^\\\\u0041-\\\\u005A\\\\u0061-\\\\u007A]','', text))\n",
    "    non_english_alphabets_removed = re.findall(r'[^\\\\u0041-\\\\u005A\\\\u0061-\\\\u007A]', text)[:50]\n",
    "    print(\"First 100 non-English alphabetic characters removed:\", non_english_alphabets_removed)\n",
    "\n",
    "\n",
    "    # Update total counts\n",
    "    self.total_link_count += link_count\n",
    "    self.total_word_with_num_count += word_with_num_count\n",
    "    self.total_whitespace_removed_count += whitespace_removed_count\n",
    "    self.total_punctuation_removed_count += punctuation_removed_count\n",
    "    self.total_non_english_alphabet_removed_count += non_english_alphabet_removed_count\n",
    "\n",
    "\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def print_total_counts(self):\n",
    "    print(\"Total counts:\")\n",
    "    print(f\"Link count: {self.total_link_count}\")\n",
    "    print(f\"Word with number count: {self.total_word_with_num_count}\")\n",
    "    print(f\"Whitespace removed count: {self.total_whitespace_removed_count}\")\n",
    "    print(f\"Punctuation removed count: {self.total_punctuation_removed_count}\")\n",
    "    print(f\"Other Languages alphabet detect: {self.total_non_english_alphabet_removed_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
