{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custom_tuned_model (train_path, test_path, language):\n",
    "    \n",
    "    #read train & test file\n",
    "    df_train = pd.read_csv(train_path, encoding='utf-8')\n",
    "    df_test = pd.read_csv(test_path, encoding='utf-8')\n",
    "\n",
    "     # Replace NaN with an empty string and ensure all text is treated as string\n",
    "    df_train['tweet_text'] = df_train['tweet_text'].fillna('').astype(str)\n",
    "    df_test['tweet_text'] = df_test['tweet_text'].fillna('').astype(str)\n",
    "    \n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = 'xlm-roberta-base'\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)  # binary classification\n",
    "\n",
    "    #tokenize_visualize(df_train, tokenizer)\n",
    "    #tokenize_visualize(df_test, tokenizer)\n",
    "\n",
    "    train_texts = df_train[\"tweet_text\"]\n",
    "    train_labels = df_train[\"class_label\"]\n",
    "    test_texts = df_test[\"tweet_text\"]\n",
    "    test_labels = df_test[\"class_label\"]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_labels)\n",
    "    train_labels_num = le.transform(train_labels)\n",
    "    le.fit(test_labels)\n",
    "    test_labels_num = le.transform(test_labels)\n",
    "\n",
    "    train_dataset = CustomDataset(train_texts, train_labels_num, tokenizer, max_length=512)\n",
    "    val_dataset = CustomDataset(test_texts, test_labels_num, tokenizer, max_length=512)\n",
    "\n",
    "    # Define DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "    \n",
    "    # Optimizer & Learning Rate Scheduler\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 2e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    # check if we have cuda installed\n",
    "    if torch.cuda.is_available():\n",
    "        # to use GPU\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('GPU is:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            labels = labels.to(torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                labels = labels.to(torch.long)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{3}, Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(\"../fine_tuned_models/fine_tuned_xlm_roberta_model_\"+language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_visualize (df, tokenizer):\n",
    "    tokenized_feature_raw = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            df[\"tweet_text\"].values.tolist(),\n",
    "                            add_special_tokens = True\n",
    "                   )\n",
    "    # collect tokenized sentence length\n",
    "    token_sentence_length = [len(x) for x in tokenized_feature_raw['input_ids']]\n",
    "    print('max: ', max(token_sentence_length))\n",
    "    print('min: ', min(token_sentence_length))\n",
    "    \n",
    "    # plot the distribution\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.hist(token_sentence_length, rwidth = 0.9)\n",
    "    plt.xlabel('Sequence Length', fontsize = 18)\n",
    "    plt.ylabel('# of Samples', fontsize = 18)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Call all language file to generate custom fine tuned model by tuning transformer model\n",
    "\n",
    "generate_custom_tuned_model(\"CT24_checkworthy_spanish_train_preprocessed.csv\", \"CT24_checkworthy_spanish_dev_preprocessed.csv\", \"spanish\")\n",
    "generate_custom_tuned_model(\"CT24_checkworthy_arabic_train_preprocessed.csv\", \"CT24_checkworthy_arabic_dev_preprocessed.csv\", \"arabic\")\n",
    "generate_custom_tuned_model(\"CT24_checkworthy_dutch_train_preprocessed.csv\", \"CT24_checkworthy_dutch_dev_preprocessed.csv\", \"dutch\")\n",
    "generate_custom_tuned_model(\"CT24_checkworthy_english_train_preprocessed.csv\", \"CT24_checkworthy_english_dev_preprocessed.csv\", \"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
